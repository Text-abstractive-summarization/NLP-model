{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"mt5_small(import MT5).ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-WeWIu60yp2D","executionInfo":{"elapsed":19831,"status":"ok","timestamp":1637644140786,"user":{"displayName":"배송이","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09026086329429757557"},"user_tz":-540},"outputId":"502f6820-7a1a-485f-c5d8-74dca1e55886"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"23RcYKGZFSro"},"source":["참고 url : https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JR-heIHg7zJ4","executionInfo":{"elapsed":12146,"status":"ok","timestamp":1637644181889,"user":{"displayName":"배송이","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09026086329429757557"},"user_tz":-540},"outputId":"af348d7d-339e-4b70-858f-64d3705e29ff"},"source":["!pip install transformers\n","!pip install sentencepiece==0.1.91"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 5.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 54.4 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 33.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 39.3 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n","Collecting sentencepiece==0.1.91\n","  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 5.1 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.91\n"]}]},{"cell_type":"markdown","metadata":{"id":"lPfEMbfCEo5u"},"source":["런타임 다시 시작 눌러야할 수 있음  \n","만일 tokenizer Nonetype 에러시 런타임 다시 시작"]},{"cell_type":"code","metadata":{"id":"V43hOR_o8t8Y"},"source":["# model.generate(pieces)\n","from transformers import MT5Config, MT5Tokenizer, MT5ForConditionalGeneration\n","from tqdm import tqdm\n","import torch\n","from torch.utils.data import DataLoader\n","\n","model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n","tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e34lHWGtsSWd"},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LeNA2dpdtAVi"},"source":["class CustomDataset:\n","\n","    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.source_len = source_len\n","        self.summ_len = summ_len\n","        self.text = self.data.text\n","        self.ctext = self.data.ctext\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        ctext = str(self.ctext[index])\n","        ctext = ' '.join(ctext.split())\n","\n","        text = str(self.text[index])\n","        text = ' '.join(text.split())\n","\n","        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n","        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n","\n","        source_ids = source['input_ids'].squeeze()\n","        source_mask = source['attention_mask'].squeeze()\n","        target_ids = target['input_ids'].squeeze()\n","        target_mask = target['attention_mask'].squeeze()\n","\n","        return {\n","            'source_ids': source_ids.to(dtype=torch.long), \n","            'source_mask': source_mask.to(dtype=torch.long), \n","            'target_ids': target_ids.to(dtype=torch.long),\n","            'target_ids_y': target_ids.to(dtype=torch.long)\n","        }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zcvmRVxasS_d"},"source":["\n","\n","def train(epoch, tokenizer, model, device, loader, optimizer):\n","    model.train()\n","    for _,data in tqdm(enumerate(loader, 0)):\n","        y = data['target_ids'].to(device, dtype = torch.long)\n","        y_ids = y[:, :-1].contiguous()\n","        lm_labels = y[:, 1:].clone().detach()\n","        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n","        ids = data['source_ids'].to(device, dtype = torch.long)\n","        mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n","        loss = outputs[0]\n","        \n","        if _%10 == 0:\n","            pass\n","            \n","        if _%500==0:\n","            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # xm.optimizer_step(optimizer)\n","        # xm.mark_step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ww9SpZIhsS8O"},"source":["def validate(epoch, tokenizer, model, device, loader):\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","    with torch.no_grad():\n","        for _, data in enumerate(loader, 0):\n","            y = data['target_ids'].to(device, dtype = torch.long)\n","            ids = data['source_ids'].to(device, dtype = torch.long)\n","            mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","            generated_ids = model.generate(\n","                input_ids = ids,\n","                attention_mask = mask, \n","                max_length=150, \n","                num_beams=2,\n","                repetition_penalty=2.5, \n","                length_penalty=1.0, \n","                early_stopping=True\n","                )\n","            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n","            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n","            if _%100==0:\n","                print(f'Completed {_}')\n","\n","            predictions.extend(preds)\n","            actuals.extend(target)\n","    return predictions, actuals"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Li1ZjzuosS5o","executionInfo":{"elapsed":398,"status":"ok","timestamp":1637645262994,"user":{"displayName":"배송이","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09026086329429757557"},"user_tz":-540},"outputId":"8c4c1559-0fd6-44d4-bf15-0a19dd2d97df"},"source":["model.to(device)"],"execution_count":null,"outputs":[{"data":{"text/plain":["MT5ForConditionalGeneration(\n","  (shared): Embedding(250112, 512)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(250112, 512)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 6)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedGeluDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedGeluDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedGeluDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedGeluDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedGeluDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedGeluDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedGeluDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedGeluDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(250112, 512)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","              (relative_attention_bias): Embedding(32, 6)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedGeluDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedGeluDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedGeluDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedGeluDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedGeluDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedGeluDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedGeluDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=512, out_features=384, bias=False)\n","              (k): Linear(in_features=512, out_features=384, bias=False)\n","              (v): Linear(in_features=512, out_features=384, bias=False)\n","              (o): Linear(in_features=384, out_features=512, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedGeluDense(\n","              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n","              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n","              (wo): Linear(in_features=1024, out_features=512, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",")"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"HPz-4_59r3Jt"},"source":["hyper-parameters"]},{"cell_type":"code","metadata":{"id":"UbU1ySuJsS36"},"source":["config = MT5Config()\n","config.MAX_LEN = 1024\n","config.SUMMARY_LEN = 150 \n","config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n","config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n","config.TRAIN_EPOCHS = 3        # number of epochs to train (default: 10)\n","config.VAL_EPOCHS = 1 \n","config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n","config.SEED = 42               # random seed (default: 42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hLshDtNsS1R"},"source":["train_params = {\n","        'batch_size': config.TRAIN_BATCH_SIZE,\n","        'shuffle': True,\n","        'num_workers': 0\n","        }\n","\n","val_params = {\n","        'batch_size': config.VALID_BATCH_SIZE,\n","        'shuffle': False,\n","        'num_workers': 0\n","        }\n","\n","optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xrru0dvtxaWA","executionInfo":{"elapsed":333,"status":"ok","timestamp":1637645273032,"user":{"displayName":"배송이","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09026086329429757557"},"user_tz":-540},"outputId":"995bf300-2e22-4c00-913c-b6e0ced5e1f4"},"source":["!nvidia-smi\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Tue Nov 23 05:27:51 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P0    35W / 250W |  16217MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"1z5DClk92JFV"},"source":["import pandas as pd\n","train_dataset = pd.read_csv('/content/drive/MyDrive/3차 프로젝트/dataset/extract_data/train_data.csv')[['document','label']]\n","validation_dataset = pd.read_csv('/content/drive/MyDrive/3차 프로젝트/dataset/extract_data/validation_data.csv')[['document','label']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9tOihkaIsAAq"},"source":["partial dataset\n"]},{"cell_type":"code","metadata":{"id":"stFdG8LTrseJ"},"source":["import numpy as np\n","\n","train_dataset = train_dataset.sample(frac=1).reset_index(drop=True).iloc[:2000]\n","validation_dataset = validation_dataset.sample(frac=1).reset_index(drop=True).iloc[:500]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I4JFTHoisIGQ"},"source":["train"]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"U7e4LNJssSzF","executionInfo":{"elapsed":707455,"status":"ok","timestamp":1637645999409,"user":{"displayName":"배송이","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09026086329429757557"},"user_tz":-540},"outputId":"97ca9b6a-b4b3-41df-a7a3-bffd3ad44b8f"},"source":["train_dataset.columns = ['ctext','text']\n","train_dataset.ctext = 'summarize: ' + train_dataset.ctext\n","\n","training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","\n","training_loader = DataLoader(training_set, **train_params)\n","\n","optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n","for epoch in range(config.TRAIN_EPOCHS):\n","    print (1)\n","    train(epoch, tokenizer, model, device, training_loader, optimizer)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["1\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0, Loss:  25.1658878326416\n"]},{"name":"stderr","output_type":"stream","text":["500it [59:27,  6.83s/it]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0, Loss:  3.5197627544403076\n"]},{"name":"stderr","output_type":"stream","text":["1000it [1:55:48,  6.95s/it]\n"]},{"name":"stdout","output_type":"stream","text":["1\n"]},{"name":"stderr","output_type":"stream","text":["\r0it [00:00, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1, Loss:  1.959478497505188\n"]},{"name":"stderr","output_type":"stream","text":["304it [34:06,  6.77s/it]"]}]},{"cell_type":"markdown","metadata":{"id":"G2OKiEwzsKTs"},"source":["valid(test)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":535},"id":"h0ly0z2xsSwq","executionInfo":{"elapsed":225861,"status":"ok","timestamp":1637646490049,"user":{"displayName":"배송이","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09026086329429757557"},"user_tz":-540},"outputId":"a511183d-543d-4583-f39d-fd404c35b2b6"},"source":["\n","\n","validation_dataset.columns = ['ctext','text']\n","validation_dataset.ctext = 'summarize: ' + validation_dataset.ctext\n","\n","val_set = CustomDataset(validation_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","\n","val_loader = DataLoader(val_set, **val_params)\n","\n","for epoch in range(config.VAL_EPOCHS):\n","    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n","    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n","\n","final_df"],"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"name":"stdout","output_type":"stream","text":["Completed 0\n","Completed 100\n","Completed 200\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Generated Text</th>\n","      <th>Actual Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>&lt;extra_id_0&gt; 데이터를 공유했을 때 사람들의 행동 변화를 이끌어낼 수 있다...</td>\n","      <td>스마트시티 분야 권위자인 MIT 교수는 도시와 사회에 미치는 데이터의 힘을 강조하였...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>&lt;extra_id_0&gt;다.</td>\n","      <td>오너 일가의 상속세 납부를 위해 LG는 2017년까지 주당 배당금 1300원을 20...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>&lt;extra_id_0&gt;하고 있다.</td>\n","      <td>삼성전자는 교원웰스를 통해 자사의 비스포크 김치냉장고를 렌탈 판매할 예정이며 이로써...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>&lt;extra_id_0&gt;을 보고받고, A 재활용업체가 불법으로 방치한 폐기물 처리 현...</td>\n","      <td>환경부 조명래 장관과 김주수 의성군수는 21일 경북 의성군 폐기물 관리법을 위반한 ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>&lt;extra_id_0&gt; 1호로 관심을 모았던 국회 수소충전소가 문을 연다.</td>\n","      <td>산업통상자원부는 10일 산업융합 분야 규제 샌드박스 1호로 관심을 모았던 국회 수소...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>495</th>\n","      <td>&lt;extra_id_0&gt; 등 다양한 방법으로 진행된다.</td>\n","      <td>홈플러스가 오는 17일까지 대규모 온라인 사은행사인 '땡Q 페스티벌'을 계획하고 할...</td>\n","    </tr>\n","    <tr>\n","      <th>496</th>\n","      <td>&lt;extra_id_0&gt; 추모 음악회'와 '김영의 선생님 사진·유물전' 개최된다.</td>\n","      <td>이화여자대학교와 음악대학이 본교 창립 133주년 및 故 김영의 선생(1908~198...</td>\n","    </tr>\n","    <tr>\n","      <th>497</th>\n","      <td>&lt;extra_id_0&gt;며 \"그런 식의 이야기를 한다\"고 밝혔다.</td>\n","      <td>16일 박지원 민주평화당 의원은 tbs 라디오에서 손학규 바른미래당 대표가 민평당 ...</td>\n","    </tr>\n","    <tr>\n","      <th>498</th>\n","      <td>&lt;extra_id_0&gt;하는 취지로 작년부터 매년 100개 기업을 선정했다.</td>\n","      <td>LED 조명전문기업 글로우원은 8일 고용노동부가 주관하는 2019 대한민국 일자리 ...</td>\n","    </tr>\n","    <tr>\n","      <th>499</th>\n","      <td>&lt;extra_id_0&gt; 한정)이 준비됐다.</td>\n","      <td>'2019 FIFA 프랑스 여자월드컵TM'을 앞두고, 개최국 프랑스가 'FIFA공식...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>500 rows × 2 columns</p>\n","</div>"],"text/plain":["                                        Generated Text                                        Actual Text\n","0    <extra_id_0> 데이터를 공유했을 때 사람들의 행동 변화를 이끌어낼 수 있다...  스마트시티 분야 권위자인 MIT 교수는 도시와 사회에 미치는 데이터의 힘을 강조하였...\n","1                                       <extra_id_0>다.  오너 일가의 상속세 납부를 위해 LG는 2017년까지 주당 배당금 1300원을 20...\n","2                                   <extra_id_0>하고 있다.  삼성전자는 교원웰스를 통해 자사의 비스포크 김치냉장고를 렌탈 판매할 예정이며 이로써...\n","3    <extra_id_0>을 보고받고, A 재활용업체가 불법으로 방치한 폐기물 처리 현...  환경부 조명래 장관과 김주수 의성군수는 21일 경북 의성군 폐기물 관리법을 위반한 ...\n","4            <extra_id_0> 1호로 관심을 모았던 국회 수소충전소가 문을 연다.  산업통상자원부는 10일 산업융합 분야 규제 샌드박스 1호로 관심을 모았던 국회 수소...\n","..                                                 ...                                                ...\n","495                      <extra_id_0> 등 다양한 방법으로 진행된다.  홈플러스가 오는 17일까지 대규모 온라인 사은행사인 '땡Q 페스티벌'을 계획하고 할...\n","496       <extra_id_0> 추모 음악회'와 '김영의 선생님 사진·유물전' 개최된다.  이화여자대학교와 음악대학이 본교 창립 133주년 및 故 김영의 선생(1908~198...\n","497                <extra_id_0>며 \"그런 식의 이야기를 한다\"고 밝혔다.  16일 박지원 민주평화당 의원은 tbs 라디오에서 손학규 바른미래당 대표가 민평당 ...\n","498          <extra_id_0>하는 취지로 작년부터 매년 100개 기업을 선정했다.  LED 조명전문기업 글로우원은 8일 고용노동부가 주관하는 2019 대한민국 일자리 ...\n","499                            <extra_id_0> 한정)이 준비됐다.  '2019 FIFA 프랑스 여자월드컵TM'을 앞두고, 개최국 프랑스가 'FIFA공식...\n","\n","[500 rows x 2 columns]"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IHoDEMB8YCIP","executionInfo":{"status":"ok","timestamp":1637649500151,"user_tz":-540,"elapsed":373,"user":{"displayName":"배송이","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09026086329429757557"}},"outputId":"b993d9a6-829d-4d32-e350-c0a4aec371ef"},"source":["for i in range(100):\n","  print(final_df['Generated Text'][i])"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["<extra_id_0> 데이터를 공유했을 때 사람들의 행동 변화를 이끌어낼 수 있다고 설명했다.\n","<extra_id_0>다.\n","<extra_id_0>하고 있다.\n","<extra_id_0>을 보고받고, A 재활용업체가 불법으로 방치한 폐기물 처리 현장을 살폈다.\n","<extra_id_0> 1호로 관심을 모았던 국회 수소충전소가 문을 연다.\n","<extra_id_0> 시대와 미래에 대비할 수 있는 '수원형 인구정책'을 수립하고 있다.\n","<extra_id_0>을 기반으로 자동 상담을 지원하는 민원서비스가 개발됐다.\n","<extra_id_0> 등 총 28개반을 현장에 투입하고, 중앙본부 특별점검반 1대 추가 운영 등 총 28개반을 현장에 투입하고, 중앙본부 특별점검반 1대 추가 운영 등 총 28개반을 현장에 투입하고, 중앙본부 특별점검반 1대 추가 운영 등 총 28개반을 현장에 투입하고, 중앙본부 특별점검반 1대 추가 운영 등 총 28개반을 현장에 투입하고, 중앙본부 특별점검반 1대 추가 운영 등 총 28개반을 현장에 투입하고, 중앙본부 특별점검반 1대 추가 운영 등 총 28개반을 현장에 투\n","<extra_id_0> 등 다양한 기후기술과 기술이 적용된 시제품, 상용제품을 선보인다.\n","<extra_id_0> 고장 문제로 도마에 오르고 있다.\n","<extra_id_0> 연구의 권위자로 약물전달시스템 분야 연구의 권위자로 약물전달시스템 분야 연구의 권위자로 약물전달시스템 분야 연구의 권위자로 약물전달시스템 분야 연구의 권위자로 약물전달시스템 분야 연구의 권위자로 약물전달시스템 분야 연구의 권위자로 약물전달시스템 분야 연구의 권위자로 약물전달시스템 분야 연구의 권위자로 약물전달시스템 분야 연구의 권위자로 약물전달시스템 분야 연구의 권위자로 약물전달시스템 분야 연구의 권위자로 약물전달시스템 분야 연구의 권위\n","<extra_id_0> 네트워크를 결합한 협업으로 현대글로비스가 중국을 비롯한 글로벌 사업 분야 확대를 위한 양해각서(MOU)를 체결했다고 29일 밝혔다.\n","<extra_id_0>'을 개최하고 우수사례를 시상한다고 밝혔다.\n","<extra_id_0> 류현진이 13일(한국시간) 미국 캘리포니아주 로스앤젤레스 다저스타디움에서 열린 2019 미국프로야구 메이저리그 워싱턴 내셔널스와 홈경기에 선발 등판했다.\n","<extra_id_0>을 선발해 교육시킨 후 졸업과 동시에 기업이 채용하는 시스템으로 별도의 정원으로 운영된다.\n","<extra_id_0> 등의 문제가 발생하는 경우 해당 기업이 속한 국가에 공식적으로 이를 전달할 창구도 미흡하다.\n","<extra_id_0>과 정신질환자 등에 의료·복지 등 각종 서비스를 연계한 공공임대주택을 공급한다.\n","<extra_id_0> 이에 대한 해결책으로 보청기를 찾게 되는 경우가 늘어나고 있다.\n","<extra_id_0>을 선언해 화웨이가 글로벌 시장에서 고립 위기에 처했다.\n","<extra_id_0> 지난해 7월 '보이 보이 보이 보이(BOY BOY BOY)'로 두 번째 프리뷰 앨범을 선보였다.\n","<extra_id_0>, 국회 정상화가 목전에 왔다는 정치권의 기대와 달리 한국당은 본회의와 추경 심사에 대해 강한 반대 입장을 유지하고 있다.\n","<extra_id_0> 등을 들 수 있는데, 개인활동에 대한 소비트랜드가 변화되며 자신에게 투자하는 시간을 증가하는 추세를 보이고 있다.\n","<extra_id_0>에 대한 의견을 교환하는 자리가 될 것으로 보인다.\n","<extra_id_0>했다.\n","<extra_id_0>인 고호석 전 부마민주항쟁기념재단 상임이사가 지병으로 별세했다.\n","<extra_id_0> 데이터 분석과 정체 예측이 가능하다.\n","<extra_id_0>, 스토리텔링형 문제를 통해 응시자의 사고력과 창의력, 문제해결력 등을 평가했다.\n","<extra_id_0>와 프랑스 PSA그룹이 합병에 합의했다.\n","<extra_id_0> 서비스를 개편, 확대할 예정이다.\n","<extra_id_0> 국가로 통한다는 점에서 의미가 크다.\n","<extra_id_0> 소화기계의 근본적인 문제를 개선하는 한약과 침, 약침 치료가 효과적이다.\n","<extra_id_0>며 '생리 짤'도 활발하게 공유되는 것 같다\"고 말했다.\n","<extra_id_0>을 설립하기로 했다.\n","<extra_id_0>을 열고 닫아 핸드폰을 자동으로 거치할 수 있다.\n","<extra_id_0> 사망자는 모두 69명인 것으로 집계됐다.\n","<extra_id_0> 지지층이 이탈했고 정치혐오에만 빠진 상황이다.\n","<extra_id_0>했다.\n","<extra_id_0>을 기록하면서 긍정평가의 격차는 불과 4.4%포인트로 좁혀졌다.\n","<extra_id_0> 보고를 드리지 않기로 했다.\n","<extra_id_0>한 첫 사례다.\n","<extra_id_0>으로 일정이 미뤄진 상태다.\n","<extra_id_0>을 당했다.\n","<extra_id_0> 등 고객 수요 맞춤형 VR 교육훈련 플랫폼 사업을 본격화한다.\n","<extra_id_0> 등 사업구조 개선을 통해 지속적인 실적 개선을 꿰한다는 목표다.\n","<extra_id_0> 등 큰 부담이 발생한다는 입장을 내놨다.\n","<extra_id_0>으로 주방을 더욱 넓게 쓸 수 있도록 했다.\n","<extra_id_0>한 상황에서 AI 스타트업이 사업초기에 겪는 다양한 애로사항을 접목하고 있다.\n","<extra_id_0> 30명으로 구성된 미소원정대는 현지 지역주민 2400여명을 대상으로 무료 진료와 건강교육을 실시했다.\n","<extra_id_0> 및 관련 핵심기술 선제 확보, 기능성 검증을 구현하는 첨단 R&D 파이프라인도 구축했다.\n","<extra_id_0>고 화가 난다\"고 비판했다.\n","<extra_id_0>된다.\n","<extra_id_0> 이에스엠이 결국 '시간연장'을 선택하면서 향후 관심은 라이크기획과의 합병 요구에 쏠린다.\n","<extra_id_0> 경쟁사(SK이노베이션)가 스스로 잘못이 없다고 판단한다면 소송에 맞소송으로 대립하고 있다.\n","<extra_id_0> 내용의 '경기도교육청 일본 전범기업 기억에 관한 조례안'이 재추진된다.\n","<extra_id_0> 등을 갖춘 전시컨벤션센터를 건립할 계획이라고 밝혔다.\n","<extra_id_0> 플랫폼 '레뷰(Revu)' 국내 누적회원수가 50만명을 넘어섰다고 18일 밝혔다.\n","<extra_id_0> 등 관련 정보를 직접 제출해야 하는 등의 불편이 있다.\n","<extra_id_0>한 제품으로 스마트시티 시대 전자파를 감소하기 위해 필수적이다.\n","<extra_id_0>이다.\n","<extra_id_0> 등으로 구성했다.\n","<extra_id_0>을 기반으로 국내 최고수준의 클린공장을 구현하기 위함이다.\n","<extra_id_0> 등 약 20명은 유성구 장애인종합복지관에서 봉사활동을 펼치고 후원물품을 기부했다.\n","<extra_id_0> 등 사업도 진행한다.\n","<extra_id_0>의 전환\"을 강조하며 회사를 더 알게 되고, 새로운 비전을 세우게 됐다고 밝혔다.\n","<extra_id_0>을 나타내는 마텐스 경도가 10에 달한다.\n","<extra_id_0>을 위한 실천과제를 담은 자율 협약'을 체결한다고 밝혔다.\n","<extra_id_0> 사장의 '세계 변화에 따른 원자력 에너지 프로그램의 중요성' 등 기조 강연이 이어진다.\n","<extra_id_0> 등을 통해 사회진출을 희망하는 여성이 한자리에 모여 선배를 만나는 '제6회 SW 웰컴 걸스'를 개최한다.\n","<extra_id_0> 안양시가 전국최초 자체개발한 앱서비스다.\n","<extra_id_0> 장애인들이 지역사회에서 자유롭고 행복한 생활을 누릴 수 있게 거주시설의 소규모화 정책이 추진돼 왔지만 사실상 '실패'라는 연구결과가 나왔다.\n","<extra_id_0> 학습자(1927년생, 만 92세)가 졸업장을 받는다.\n","<extra_id_0>된다.\n","<extra_id_0> 유가족에 1000만원 안전보험금을 지급했다고 11일 밝혔다.\n","<extra_id_0> 및 취업역량을 배양해 민간 일자리로 연계하는 사업이다.\n","<extra_id_0> 소비자 불만도 함께 올랐다.\n","<extra_id_0> 지수현이 차우진(차순배)의 기세를 꺾는 사진을 입수하는 데 성공, 검찰청에서 풀려난 지수현과 마중 나온 한석주가 격렬 키스 엔딩'을 선사하며 안방극장을 휘몰아쳤다.\n","<extra_id_0> 골목을 행정안전부가 주관한 지역골목경제 융·복합 상권개발 사업에 공모신청하여 24일 최종 선정됐다\n","<extra_id_0>을 제공한다고 발표했다.\n","<extra_id_0>을 통한 멸균 사업 성장 도모 등이다.\n","<extra_id_0>을 소개하고 각각 다른 환경의 고객들에게 맞는 UPS 활용법과 선택법을 제안한다.\n","<extra_id_0> 조사결과가 나오고, 소비세율을 10%로 인상한 것에 대해 절반 이상의 여론이 납득한다는 조사결과가 나왔다.\n","<extra_id_0>하고, 임상시험 결과가 기대치를 밑돌았다.\n","<extra_id_0>하고, 지난해 8월에는 상수도 검침용역 근로자 185명을 정규직으로 전환하기로 합의했다.\n","<extra_id_0>'라는 이름으로 디지털 신한인 채용 위크를 진행할 예정이다.\n","<extra_id_0> 등 계량지표 측면에서 성과를 달성했다.\n","<extra_id_0>을 강조하는 추세다.\n","<extra_id_0> 세미나를 개최한다고 19일 밝혔다.\n","<extra_id_0>으로 가장 가파른 상승세를 보였다.\n","<extra_id_0>으로 은행권 가계대출 증가 규모는 전년 동월(5조원)보다 4조원 줄었다.\n","<extra_id_0> '유치원 3법'이 국회통과를 기대하고 있다.\n","<extra_id_0>을 포함한 개폐막식 이벤트를 준비 중인 것으로 알려졌다.\n","<extra_id_0> 등에 관심을 갖고 있다.\n","<extra_id_0>, 스튜디오, 영상 편집실 등 작업 공간이 구성돼 창작자〃스타트업 교육 프로그램을 운영할 계획이다.\n","<extra_id_0>이라고 말했다.\n","<extra_id_0> 방식인 온라인 선착순에서 소상공인의 편의성을 최대한 반영한 온라인 선착순 방식으로 변경해 처음 실시했다.\n","<extra_id_0> 등 여러 분야에서 모범이 되고 있는 12개사에 인증서를 수여했다.\n","<extra_id_0>을 검증할 수 있는 테스트베드를 확보하게 된다.\n","<extra_id_0> 등 다양한 기술과 서비스를 활용하면 한층 빠르게 비즈니스 규모를 확장할 수 있을 것이라고 밝혔다.\n","<extra_id_0> 사업에서 우리나라가 우선협상대상자로 선정됐다고 27일 밝혔다.\n","<extra_id_0> 이에 대해 금융당국은 전수조사 조차 하지 않고 있다.\n"]}]}]}